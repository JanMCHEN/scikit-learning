# scikit-learning
机器学习笔记，结合sklearn学习一些常见算法实现方式

## 有监督学习
    scikit调用方式汇总在supervisor_learning.ipynb内
    需提供带有标签的数据集，通常做法是将数据集分成训练数据和测试数据，通过训练数据建立一个数学模型，能很好的拟合训练数据的标签值并且能很好预测测试数据或未在训练数据中出现过的数据。其中存在欠拟合和过拟合现象。欠拟合指没能很好拟合训练数据，通常是模型过于简单。过拟合指虽能很好拟合训练数据，但对未在训练数据中出现的数据无法准确预测，通常是模型过于复杂。
### 线性模型
    求解y=Σwi*xi中各系数w，其中x0=1，w0称为截距intercept，w1-wn称为回归系数coefficient。对于样本x，预测值为yp，样本数量m，我们的目标是损失函数最小化，一般选取均方误差损失函数L(f)=Σ(y-yp)²/2m，所以目标就是w = arg(min(L(w))。
    在myfun.linear_reg*有完整代码
#### OLS
    求解上述损失函数最小化时，实际上是有解析解的，可以用最小二乘法求解。L对w求导令倒数为0，得到解析解，通常在样本数量大于等于特征数量时有唯一解，否则存在多个解析解，此时需要引入正则化项，如L1、L2 。此方法在样本特征维数增多时模型求得的w也显著增大，原因是为了更好的拟合训练数据中很小的x值差异产生较大的y值差异，w必须增大，造成了任何一个特征微小的变化都会导致预测值大的变化，这就造成了过拟合。
#### 岭回归
    损失函数引入L2=αΣw²正则项，既可以解决OLS中样本数不足时的多个解析解问题，还能防止过拟合，控制模型空间，其中α越大，正则化项影响越大，模型越简单，极端情况α趋于无穷时w=0，此时模型就变成了一条水平直线，性能自然就变得差了。所以α的选取很关键，一般控制小于1。
#### Lasso回归
    引入L1 = αΣ|w|，可以把控制系数收缩到0从而达到变量选择的目的
#### Elastic回归
    同时引入L1和L2
#### 梯度下降
    梯度下降不同于OLS，它是一个通过梯度不断迭代更新参数的过程，用途比较广，要注意特征归1化。
#### 广义线性模型
    考虑单调可导函数h(·),先对y求h（y),再利用线性模型。实质上这不是线性的，但可以通过数据预处理转换y，再在必要时进行还原。
#### Logistics回归
    逻辑回归，个人觉得相当于深度学习中的全连接层加sigmoid函数，首先它是针对分类任务的，一般求解用梯度下降方法。